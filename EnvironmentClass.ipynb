{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator, sys, os\n",
    "import tensorflow as tf\n",
    "import plotting\n",
    "import itertools\n",
    "import matplotlib\n",
    "import collections\n",
    "import random\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insured_neg_question_table = np.array([\n",
    "                              [\"insured_neg_yng_adult_life\",\"insured_neg_yng_adult_car\",\"insured_neg_yng_adult_dental\",\"insured_neg_yng_adult_health\",\"insured_neg_yng_adult_disability\"],\n",
    "                              [\"insured_neg_middle-age_life\",\"insured_neg_middle-age_car\",\"insured_neg_middle-age_dental\",\"insured_neg_middle-age_health\",\"insured_neg_middle-age_disability\"],\n",
    "                              [\"insured_neg_old-age_life\",\"insured_neg_old-age_car\",\"insured_neg_old-age_dental\",\"insured_neg_old-age_health\",\"insured_neg_old-age_disability\"]\n",
    "                               ])\n",
    "\n",
    "insured_pos_question_table = np.array([\n",
    "                              [\"insured_pos_yng_adult_life\",\"insured_pos_yng_adult_car\",\"insured_pos_yng_adult_dental\",\"insured_pos_yng_adult_health\",\"insured_pos_yng_adult_disability\"],\n",
    "                              [\"insured_pos_middle-age_life\",\"insured_pos_middle-age_car\",\"insured_pos_middle-age_dental\",\"insured_pos_middle-age_health\",\"insured_pos_middle-age_disability\"],\n",
    "                              [\"insured_pos_old-age_life\",\"insured_pos_old-age_car\",\"insured_pos_old-age_dental\",\"insured_pos_old-age_health\",\"insured_pos_old-age_disability\"]\n",
    "                               ])\n",
    "\n",
    "open_question_table = np.array([\n",
    "                              [\"open_yng_adult_life\",\"open_yng_adult_car\",\"open_yng_adult_dental\",\"open_yng_adult_health\",\"open_yng_adult_disability\"],\n",
    "                              [\"open_middle-age_life\",\"open_middle-age_car\",\"open_middle-age_dental\",\"open_middle-age_health\",\"open_middle-age_disability\"],\n",
    "                              [\"open_old-age_life\",\"open_old-age_car\",\"open_old-age_dental\",\"open_old-age_health\",\"open_old-age_disability\"]\n",
    "                               ])\n",
    "\n",
    "\n",
    "\n",
    "def get_action_dict(open_question_table,insured_pos_question_table,insured_neg_question_table):\n",
    "    \n",
    "    action_dict = dict({\"open\":{\"young_adult\": {val:idx for idx, val in enumerate(open_question_table[0])},\n",
    "                \"middle_age\":{val:idx for idx, val in enumerate(open_question_table[1],len(open_question_table[1]))},\n",
    "                \"old-age\":{val:idx for idx, val in enumerate(open_question_table[2],2*len(open_question_table[2]))}},\n",
    "         \"insured_pos\":{\"young_adult\": {val:idx for idx, val in enumerate(insured_pos_question_table[0],3*len(insured_pos_question_table[2]))},\n",
    "                \"middle_age\":{val:idx for idx, val in enumerate(insured_pos_question_table[1],4*len(insured_pos_question_table[1]))},\n",
    "                \"old-age\":{val:idx for idx, val in enumerate(insured_pos_question_table[2],5*len(insured_pos_question_table[2]))}},\n",
    "         \"insured_neg\":{\"young_adult\": {val:idx for idx, val in enumerate(insured_neg_question_table[0],6*len(insured_neg_question_table[0]))},\n",
    "                \"middle_age\":{val:idx for idx, val in enumerate(insured_neg_question_table[1],7*len(insured_neg_question_table[1]))},\n",
    "                \"old-age\":{val:idx for idx, val in enumerate(insured_neg_question_table[2],8*len(insured_neg_question_table[2]))}},\n",
    "                       \"close\":{\"all\":{\"Is the Customer Interested?\":45}} })\n",
    "    return action_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class User:\n",
    "    def __init__(self, user_class, action_dict):\n",
    "        self.name=\"Simulated-user\"\n",
    "        self.user_class = user_class\n",
    "        self.state = 0\n",
    "        self.action_dict = action_dict\n",
    "        self.prev_response = 0\n",
    "        self.is_insured = 0\n",
    "        \n",
    "    def print_user_speech(self):\n",
    "        if self.prev_response == 1:\n",
    "            print(\"USER SAID YES\")\n",
    "        elif self.prev_response == 0:\n",
    "            print(\"USER SAID NO\")\n",
    "            \n",
    "        \n",
    "    def respond(self,action):\n",
    "        if (self.user_class == 0):\n",
    "            if (self.state == 0):\n",
    "                if (action in self.action_dict['open']['young_adult'].values()):\n",
    "                    self.state = 1\n",
    "                    self.prev_response = np.random.choice([0,1], p=[0.5,0.5])\n",
    "                    if self.prev_response == 0:\n",
    "                        self.is_insured = 0\n",
    "                    else:\n",
    "                        self.is_insured = 1\n",
    "                    reward = 0\n",
    "                    self.print_user_speech()\n",
    "                    return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                else:\n",
    "                    print(\"User said I DID NOT UNDERSTAND\",action)\n",
    "                    reward = -1\n",
    "                    self.state = 0\n",
    "                    self.is_insured = 0\n",
    "                    self.prev_response = 0\n",
    "\n",
    "                    return (self.state, self.prev_response,self.is_insured, reward)\n",
    "            elif ((self.state == 1) and (self.is_insured == 1)):\n",
    "                if (action in self.action_dict['insured_pos']['young_adult'].values()):\n",
    "                    self.state = 2\n",
    "                    self.prev_response = np.random.choice([0,1], p=[0.2,0.8])\n",
    "                    self.print_user_speech()\n",
    "                    reward = 0\n",
    "                    return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                else:\n",
    "                    print(\"User said I DID NOT UNDERSTAND\",action)\n",
    "                    reward = -1\n",
    "                    self.state = 1\n",
    "                    self.prev_response = 0\n",
    "\n",
    "                    return (self.state, self.prev_response,self.is_insured, reward)\n",
    "            elif ((self.state == 1) and (self.is_insured == 0)):\n",
    "                if (action in self.action_dict['insured_neg']['young_adult'].values()):\n",
    "                    self.state = 2\n",
    "                    self.prev_response = np.random.choice([0,1], p=[0.2,0.8])\n",
    "                    self.print_user_speech()\n",
    "                    reward = 0\n",
    "                    return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                else:\n",
    "                    print(\"User said I DID NOT UNDERSTAND\",action)\n",
    "                    reward = -1\n",
    "                    self.state = 1\n",
    "                    self.prev_response = 0\n",
    " \n",
    "                    return (self.state, self.prev_response,self.is_insured, reward)\n",
    "            elif (self.state == 2):\n",
    "                if (action in self.action_dict['close']['all'].values()):\n",
    "                    print(\"[Close] State reached; Need action: 45, Got action\",action)\n",
    "                    if self.prev_response == 1:\n",
    "                        self.state = 4\n",
    "                        reward = 1\n",
    "                    else:\n",
    "                        self.state = 3\n",
    "                        reward = 1\n",
    "                    self.print_user_speech()\n",
    "                    \n",
    "                    return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                else:\n",
    "                    print(\"User said I DID NOT UNDERSTAND\",action)\n",
    "                    reward = -1\n",
    "                    self.state = 2\n",
    "                    self.prev_response = 0\n",
    "       \n",
    "                    return (self.state, self.prev_response,self.is_insured, reward)\n",
    "        elif (self.user_class == 1):\n",
    "                    if (self.state == 0):\n",
    "                        if (action in self.action_dict['open']['middle_age'].values()):\n",
    "                            self.state = 1\n",
    "                            self.prev_response = np.random.choice([0,1], p=[0.5,0.5])\n",
    "                            if self.prev_response == 0:\n",
    "                                self.is_insured = 0\n",
    "                            else:\n",
    "                                self.is_insured = 1\n",
    "                            reward = 0\n",
    "                            self.print_user_speech()\n",
    "                            return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                        else:\n",
    "                            print(\"User said I DID NOT UNDERSTAND\",action)\n",
    "                            reward = 0\n",
    "                            self.state = 3\n",
    "                            self.prev_response = 0\n",
    "\n",
    "                            return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                    elif ((self.state == 1) and (self.is_insured == 1)):\n",
    "                        if (action in self.action_dict['insured_pos']['middle_age'].values()):\n",
    "                            self.state = 2\n",
    "                            self.prev_response = np.random.choice([0,1], p=[0.2,0.8])\n",
    "                            self.print_user_speech()\n",
    "                            reward = 0\n",
    "                            return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                        else:\n",
    "                            print(\"User said I DID NOT UNDERSTAND\",action)\n",
    "                            reward = -1\n",
    "                            self.state = 3\n",
    "                            self.prev_response = 0\n",
    "\n",
    "                            return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                    elif ((self.state == 1) and (self.is_insured == 0)):\n",
    "                        if (action in self.action_dict['insured_neg']['middle_age'].values()):\n",
    "                            self.state = 2\n",
    "                            self.prev_response = np.random.choice([0,1], p=[0.2,0.8])\n",
    "                            self.print_user_speech()\n",
    "                            reward = 0\n",
    "                            return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                        else:\n",
    "                            print(\"User said I DID NOT UNDERSTAND\",action)\n",
    "                            reward = -1\n",
    "                            self.state = 3\n",
    "                            self.prev_response = 0\n",
    "\n",
    "                            return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                    elif (self.state == 2):\n",
    "                        if (action in self.action_dict['close']['all'].values()):\n",
    "                            print(\"[Close] State reached; Need action: 45, Got action\",action)\n",
    "                            if self.prev_response == 1:\n",
    "                                self.state = 4\n",
    "                                reward = 1\n",
    "                            else:\n",
    "                                self.state = 3\n",
    "                                reward = -1\n",
    "                            self.print_user_speech()\n",
    "\n",
    "                            return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                        else:\n",
    "                            print(\"User said I DID NOT UNDERSTAND\",action)\n",
    "                            reward = -1\n",
    "                            self.state = 3\n",
    "                            self.prev_response = 0\n",
    "\n",
    "                            return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                    else:\n",
    "                        print(\"Something went wrong Inside. Cirrent State:\",self.state)\n",
    "\n",
    "        elif (self.user_class == 2):\n",
    "                    if (self.state == 0):\n",
    "                        if (action in self.action_dict['open']['old_age'].values()):\n",
    "                            self.state = 1\n",
    "                            self.prev_response = np.random.choice([0,1], p=[0.5,0.5])\n",
    "                            if self.prev_response == 0:\n",
    "                                self.is_insured = 0\n",
    "                            else:\n",
    "                                self.is_insured = 1\n",
    "                            reward = 0\n",
    "                            self.print_user_speech()\n",
    "                            return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                        else:\n",
    "                            print(\"User said I DID NOT UNDERSTAND\",action)\n",
    "                            reward = 0\n",
    "                            self.state = 0\n",
    "                            self.prev_response = 0\n",
    "\n",
    "                            return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                    elif ((self.state == 1) and (self.is_insured == 1)):\n",
    "                        if (action in self.action_dict['insured_pos']['old_age'].values()):\n",
    "                            self.state = 2\n",
    "                            self.prev_response = np.random.choice([0,1], p=[0.2,0.8])\n",
    "                            self.print_user_speech()\n",
    "                            reward = 0\n",
    "                            return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                        else:\n",
    "                            print(\"User said I DID NOT UNDERSTAND\",action)\n",
    "                            reward = -1\n",
    "                            self.state = 1\n",
    "                            self.prev_response = 0\n",
    "\n",
    "                            return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                    elif ((self.state == 1) and (self.is_insured == 0)):\n",
    "                        if (action in self.action_dict['insured_neg']['old_age'].values()):\n",
    "                            self.state = 2\n",
    "                            self.prev_response = np.random.choice([0,1], p=[0.2,0.8])\n",
    "                            self.print_user_speech()\n",
    "                            reward = 0\n",
    "                            return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                        else:\n",
    "                            print(\"User said I DID NOT UNDERSTAND\",action)\n",
    "                            reward = -1\n",
    "                            self.state = 1\n",
    "                            self.prev_response = 0\n",
    "\n",
    "                            return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                    elif (self.state == 2):\n",
    "                        if (action in self.action_dict['close']['all'].values()):\n",
    "                            print(\"[Close] State reached; Need action: 45, Got action\",action)\n",
    "                            if self.prev_response == 1:\n",
    "                                self.state = 4\n",
    "                                reward = 1\n",
    "                            else:\n",
    "                                self.state = 3\n",
    "                                reward = -1\n",
    "                            self.print_user_speech()\n",
    "\n",
    "                            return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                        else:\n",
    "                            print(\"User said I DID NOT UNDERSTAND\",action)\n",
    "                            reward = -1\n",
    "                            self.state = 2\n",
    "                            self.prev_response = 0\n",
    "\n",
    "                            return (self.state, self.prev_response,self.is_insured, reward)\n",
    "                    else:\n",
    "                        print(\"Something went wrong Inside. Cirrent State:\",self.state)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, uclass, iclass):\n",
    "        self.name=\"Chat-Environment\"\n",
    "\n",
    "        self.user_class = uclass\n",
    "        self.item_class = iclass\n",
    "\n",
    "        self.prev_response = 0\n",
    "\n",
    "        # {open:0, ask:1, close:2}\n",
    "        self.dialog_state_dict = {\"open\":0, \"ask\":1, \"close\":2, \"closed_0\":3, \"closed_1\":4}\n",
    "        self.dialog_state = 0\n",
    "        self.is_insured = 0\n",
    "\n",
    "        # State and Action Space\n",
    "        self.action_dict = get_action_dict(open_question_table,insured_pos_question_table,insured_neg_question_table)\n",
    "\n",
    "        self.N_actions = 0\n",
    "        for key, value in self.action_dict.items():\n",
    "            for nestedkey,nestedvalue in self.action_dict[key].items():\n",
    "                    self.N_actions = (len(self.action_dict[key][nestedkey].items()) + self.N_actions)\n",
    "\n",
    "        print(\"No. actions:\", self.N_actions)\n",
    "\n",
    "        self.state_dim = len([self.user_class,\n",
    "                        self.item_class,\n",
    "                        self.prev_response,\n",
    "                        self.dialog_state,\n",
    "                        self.is_insured])\n",
    "        self.action_dim = (self.N_actions,)\n",
    "\n",
    "        self.reward = 0\n",
    "        self.state = None\n",
    "            \n",
    "            \n",
    "    def define_rewards(self):\n",
    "        if(self.dialog_state == 4):\n",
    "            reward = 1\n",
    "            return reward\n",
    "        elif(self.dialog_state == 3):\n",
    "            reward = -0.1\n",
    "            return reward\n",
    "        else:\n",
    "            reward = 0\n",
    "            return reward\n",
    "        \n",
    "    def allowed_actions(self, action_probs):\n",
    "        actions = []\n",
    "        allowed_action_probs = []\n",
    "        state = self.state\n",
    "        dialog_state = state[3]\n",
    "        if (dialog_state == 0):\n",
    "            for key, value in self.action_dict[\"open\"]['young_adult'].items():\n",
    "                actions.append(value)\n",
    "            allowed_action_probs = action_probs[actions]\n",
    "            return actions, allowed_action_probs\n",
    "        elif (dialog_state == 1 and state[2] == 1 ):\n",
    "            for key, value in self.action_dict['insured_pos']['young_adult'].items():\n",
    "                actions.append(value)\n",
    "            allowed_action_probs = action_probs[actions]\n",
    "            return actions, allowed_action_probs\n",
    "        elif (dialog_state == 1 and state[2] == 0 ):\n",
    "            for key, value in self.action_dict['insured_neg']['young_adult'].items():\n",
    "                actions.append(value)\n",
    "            allowed_action_probs = action_probs[actions]\n",
    "            return actions, allowed_action_probs\n",
    "        elif (dialog_state == 2):\n",
    "            for key, value in self.action_dict[\"close\"]['all'].items():\n",
    "                actions.append(value)\n",
    "            allowed_action_probs = action_probs[actions]\n",
    "            return actions, allowed_action_probs\n",
    "        else:\n",
    "            print(\"Dialog state is wrong or terminal state is reached\")\n",
    "            return actions, allowed_action_probs\n",
    "    \n",
    "    def reset(self):\n",
    "        print(\"_______ENV RESET___________ \")\n",
    "        self.state = [0,0,0,0,0]\n",
    "        self.dialog_state = 0\n",
    "        self.is_insured = 0\n",
    "        self.prev_response = 0\n",
    "        return np.array(self.state).flatten()\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        if((state[3] == 3) | (state[3] == 4)):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def step(self, action, user):\n",
    "        print(\"Bot said \",action)\n",
    "        next_dialog_state, prev_response,is_insured, reward = user.respond(action)\n",
    "        self.reward = reward\n",
    "        self.dialog_state = next_dialog_state\n",
    "        self.prev_response = prev_response\n",
    "        self.is_insured = is_insured\n",
    "        self.state = [self.user_class,\n",
    "                        self.item_class,\n",
    "                        self.prev_response,\n",
    "                        self.dialog_state,\n",
    "                        self.is_insured]\n",
    "        done = self.is_terminal(self.state)\n",
    "        if done == 1:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        return np.array(self.state), done, self.reward\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = Environment(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyEstimator():\n",
    "    \"\"\"\n",
    "    Policy Function approximator. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, scope=\"policy_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.float32, [None,int(env.state_dim)], \"state\")\n",
    "            self.action = tf.placeholder(dtype=tf.int32, name=\"action\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just table lookup estimator\n",
    "#             state_one_hot = tf.one_hot(self.state, int(env.state_dim))\n",
    "#             print(\"ONE HOT STATE\", state_one_hot)\n",
    "\n",
    "\n",
    "            h1 = tf.layers.dense(self.state, 24, activation=tf.nn.relu)\n",
    "            h2 = tf.layers.dense(h1, 48, activation=tf.nn.relu)\n",
    "            h3 = tf.layers.dense(h2, 24, activation=tf.nn.relu)\n",
    "            self.output_layer = tf.layers.dense(h3, env.N_actions)\n",
    "#             outputs = tf.nn.softmax(logits)\n",
    "\n",
    "#             self.output_layer = tf.contrib.layers.fully_connected(\n",
    "#                 inputs=self.state,\n",
    "#                 num_outputs=env.N_actions,\n",
    "#                 activation_fn=None,\n",
    "#                 weights_initializer=tf.zeros_initializer)\n",
    "            print(\"OUTPUT LAYER\",self.output_layer)\n",
    "\n",
    "            self.action_probs = tf.squeeze(tf.nn.softmax(self.output_layer))\n",
    "            print(\"In Estimator Action Probs:\",self.action_probs)\n",
    "            print(\"Shape\", self.action_probs.shape)\n",
    "            self.picked_action_prob = tf.gather(self.action_probs, self.action)\n",
    "\n",
    "            # Loss and train op\n",
    "            self.loss = -tf.log(self.picked_action_prob) * self.target\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        return sess.run(self.action_probs, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, action, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        feed_dict = { self.state: state, self.target: target, self.action: action  }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEstimator():\n",
    "    \"\"\"\n",
    "    Value Function approximator. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, scope=\"value_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.float32, [None,int(env.state_dim)], \"state\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "\n",
    "            \n",
    "#             state_one_hot = tf.one_hot(self.state, int(env.state_dim))\n",
    "\n",
    "            h1 = tf.layers.dense(self.state, 24, activation=tf.nn.relu)\n",
    "            h2 = tf.layers.dense(h1, 48, activation=tf.nn.relu)\n",
    "            h3 = tf.layers.dense(h2, 24, activation=tf.nn.relu)\n",
    "            self.output_layer = tf.layers.dense(h3, 1)\n",
    "\n",
    "#             self.output_layer = tf.contrib.layers.fully_connected(\n",
    "#                 inputs=self.state,\n",
    "#                 num_outputs=1,\n",
    "#                 activation_fn=None,\n",
    "#                 weights_initializer=tf.zeros_initializer)\n",
    "\n",
    "            self.value_estimate = tf.squeeze(self.output_layer)\n",
    "            self.loss = tf.squared_difference(self.value_estimate, self.target)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())        \n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        return sess.run(self.value_estimate, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        feed_dict = { self.state: state, self.target: target }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic(env, estimator_policy, estimator_value, num_episodes, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Actor Critic Algorithm. Optimizes the policy \n",
    "    function approximator using policy gradient.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        estimator_policy: Policy Function to be optimized \n",
    "        estimator_value: Value function approximator, used as a critic\n",
    "        num_episodes: Number of episodes to run for\n",
    "        discount_factor: Time-discount factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Reset the environment and pick the fisrst action\n",
    "        print(\"New Episode\", i_episode)\n",
    "        state = env.reset()\n",
    "        user = User(0,env.action_dict)\n",
    "\n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            \n",
    "            \n",
    "            action_probs = estimator_policy.predict(state.reshape([1,5]))\n",
    "            expl_chance = np.random.choice([1,0], p=[0.1,0.9])\n",
    "            if (expl_chance == 1):\n",
    "                action = np.random.choice(np.arange(len(action_probs)))\n",
    "            else:\n",
    "                action = np.argmax(action_probs)\n",
    "\n",
    "                \n",
    "#             allowed_actions, allowed_actions_probs = env.allowed_actions(action_probs)\n",
    "#             while (not action in allowed_actions):\n",
    "#                 action = np.random.choice(np.arange(len(action_probs)))\n",
    "            print(\"ENV State BEFORE Step\",state)\n",
    "            next_state, done, reward = env.step(action,user)\n",
    "            print(\"ENV State After Step\",next_state)\n",
    "            print(\"Step Rewards\", reward)\n",
    "\n",
    "            \n",
    "            # Keep track of the transition\n",
    "            episode.append(Transition(\n",
    "              state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # Calculate TD Target\n",
    "            value_next = estimator_value.predict(next_state.reshape([1,5]))\n",
    "            td_target = reward + discount_factor * value_next\n",
    "            td_error = td_target - estimator_value.predict(state.reshape([1,5]))\n",
    "            \n",
    "            # Update the value estimator\n",
    "            estimator_value.update(state.reshape([1,5]), td_target)\n",
    "            \n",
    "            # Update the policy estimator\n",
    "            # using the td error as our advantage estimate\n",
    "            estimator_policy.update(state.reshape([1,5]), td_error, action)\n",
    "            \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} @ Episode {}/{} ({})\".format(\n",
    "                    t, i_episode , num_episodes, stats.episode_rewards[i_episode]), end=\"\")\n",
    "\n",
    "            if done:\n",
    "                print(\"Final Dialog State\", env.dialog_state)\n",
    "                print(\"Episode DONE\")\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "policy_estimator = PolicyEstimator()\n",
    "value_estimator = ValueEstimator()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # Note, due to randomness in the policy the number of episodes you need to learn a good\n",
    "    # policy may vary. ~300 seemed to work well for me.\n",
    "    stats = actor_critic(env, policy_estimator, value_estimator, 1000, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_episode_stats(stats, smoothing_window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isd = np.zeros(nS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_question_table = np.array([\n",
    "                              [\"open_yng_adult_life\",\"open_yng_adult_car\",\"open_yng_adult_dental\",\"open_yng_adult_health\",\"open_yng_adult_disability\"],\n",
    "                              [\"open_middle-age_life\",\"open_middle-age_car\",\"open_middle-age_dental\",\"open_middle-age_health\",\"open_middle-age_disability\"],\n",
    "                              [\"open_old-age_life\",\"open_old-age_car\",\"open_old-age_dental\",\"open_old-age_health\",\"open_old-age_disability\"]\n",
    "                               ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insured_pos_question_table = np.array([\n",
    "                              [\"insured_pos_yng_adult_life\",\"insured_pos_yng_adult_car\",\"insured_pos_yng_adult_dental\",\"insured_pos_yng_adult_health\",\"insured_pos_yng_adult_disability\"],\n",
    "                              [\"insured_pos_middle-age_life\",\"insured_pos_middle-age_car\",\"insured_pos_middle-age_dental\",\"insured_pos_middle-age_health\",\"insured_pos_middle-age_disability\"],\n",
    "                              [\"insured_pos_old-age_life\",\"insured_pos_old-age_car\",\"insured_pos_old-age_dental\",\"insured_pos_old-age_health\",\"insured_pos_old-age_disability\"]\n",
    "                               ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insured_neg_question_table = np.array([\n",
    "                              [\"insured_neg_yng_adult_life\",\"insured_neg_yng_adult_car\",\"insured_neg_yng_adult_dental\",\"insured_neg_yng_adult_health\",\"insured_neg_yng_adult_disability\"],\n",
    "                              [\"insured_neg_middle-age_life\",\"insured_neg_middle-age_car\",\"insured_neg_middle-age_dental\",\"insured_neg_middle-age_health\",\"insured_neg_middle-age_disability\"],\n",
    "                              [\"insured_neg_old-age_life\",\"insured_neg_old-age_car\",\"insured_neg_old-age_dental\",\"insured_neg_old-age_health\",\"insured_neg_old-age_disability\"]\n",
    "                               ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = np.array([\"Is the customer interested?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actions(open_question_table,insured_pos_question_table,insured_neg_question_table):\n",
    "    \n",
    "    action_dict = dict({\"ask\":{\"young_adult\": {val:idx for idx, val in enumerate(open_question_table[0])},\n",
    "                \"middle_age\":{val:idx for idx, val in enumerate(open_question_table[1],len(open_question_table[1]))},\n",
    "                \"old-age\":{val:idx for idx, val in enumerate(open_question_table[2],2*len(open_question_table[2]))}},\n",
    "         \"insured_pos\":{\"young_adult\": {val:idx for idx, val in enumerate(insured_pos_question_table[0],3*len(insured_pos_question_table[2]))},\n",
    "                \"middle_age\":{val:idx for idx, val in enumerate(insured_pos_question_table[1],4*len(insured_pos_question_table[1]))},\n",
    "                \"old-age\":{val:idx for idx, val in enumerate(insured_pos_question_table[2],5*len(insured_pos_question_table[2]))}},\n",
    "         \"insured_neg\":{\"young_adult\": {val:idx for idx, val in enumerate(insured_neg_question_table[0],6*len(insured_neg_question_table[0]))},\n",
    "                \"middle_age\":{val:idx for idx, val in enumerate(insured_neg_question_table[1],7*len(insured_neg_question_table[1]))},\n",
    "                \"old-age\":{val:idx for idx, val in enumerate(insured_neg_question_table[2],8*len(insured_neg_question_table[2]))}}})\n",
    "    return action_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict({\"ask\":{\"user_class1\":q for q in list(open_question_table)[0:]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"open\": {\"open1\":1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dict({\"ask\":{\"young_adult\": {val:idx for idx, val in enumerate(open_question_table[0])},\n",
    "                \"middle_age\":{val:idx for idx, val in enumerate(open_question_table[1],len(open_question_table[1]))},\n",
    "                \"old-age\":{val:idx for idx, val in enumerate(open_question_table[2],2*len(open_question_table[2]))}},\n",
    "         \"insured_pos\":{\"young_adult\": {val:idx for idx, val in enumerate(insured_pos_question_table[0],3*len(insured_pos_question_table[2]))},\n",
    "                \"middle_age\":{val:idx for idx, val in enumerate(insured_pos_question_table[1],4*len(insured_pos_question_table[1]))},\n",
    "                \"old-age\":{val:idx for idx, val in enumerate(insured_pos_question_table[2],5*len(insured_pos_question_table[2]))}},\n",
    "         \"insured_neg\":{\"young_adult\": {val:idx for idx, val in enumerate(insured_neg_question_table[0],6*len(insured_neg_question_table[0]))},\n",
    "                \"middle_age\":{val:idx for idx, val in enumerate(insured_neg_question_table[1],7*len(insured_neg_question_table[1]))},\n",
    "                \"old-age\":{val:idx for idx, val in enumerate(insured_neg_question_table[2],8*len(insured_neg_question_table[2]))}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['ask']['middle_age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, val in enumerate(open_question_table[0]):\n",
    "    print(idx, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.choice([0,1], p=[0.5,0.5])\n",
    "for i in range(100000):\n",
    "    print(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_action_dict(open_question_table,insured_pos_question_table,insured_neg_question_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_actions = 0\n",
    "for key, value in a.items():\n",
    "    for nestedkey,nestedvalue in a[key].items():\n",
    "            N_actions = (len(a[key][nestedkey].items()) + N_actions)\n",
    "\n",
    "#     N_actions = (len(a[key].items()) + N_actions)\n",
    "print(\"No. actions:\", N_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a['open']['young_adult'].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = get_action_dict(open_question_table,insured_pos_question_table,insured_neg_question_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_actions = 0\n",
    "for key, value in t.items():\n",
    "    for nestedkey,nestedvalue in t[key].items():\n",
    "            N_actions = (len(t[key][nestedkey].items()) + N_actions)\n",
    "\n",
    "print(\"No. actions:\", N_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for i in range(1000):\n",
    "    print(np.random.choice([0,1], p=[0.1,0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0.3,0.5,0.6,0.34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "np.argmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
