{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers.merge import Add, Multiply\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, uclass, iclass):\n",
    "        self.name=\"Chat-Environment\"\n",
    "\n",
    "        self.user_class = uclass\n",
    "        self.item_class = iclass\n",
    "\n",
    "        self.prev_response = 0\n",
    "\n",
    "        # {open:0, ask:1, close:2}\n",
    "        self.dialog_state_dict = {\"open\":0, \"ask\":1, \"close\":2, \"closed_0\":3, \"closed_1\":4}\n",
    "        self.dialog_state = 0\n",
    "        self.is_insured = 0\n",
    "\n",
    "        # State and Action Space\n",
    "        self.action_dict = get_action_dict(open_question_table,insured_pos_question_table,insured_neg_question_table)\n",
    "\n",
    "        self.N_actions = 0\n",
    "        for key, value in self.action_dict.items():\n",
    "            for nestedkey,nestedvalue in self.action_dict[key].items():\n",
    "                    self.N_actions = (len(self.action_dict[key][nestedkey].items()) + self.N_actions)\n",
    "\n",
    "        print(\"No. actions:\", self.N_actions)\n",
    "\n",
    "        self.state_dim = len([self.user_class,\n",
    "                        self.item_class,\n",
    "                        self.prev_response,\n",
    "                        self.dialog_state,\n",
    "                        self.is_insured])\n",
    "        self.action_dim = (self.N_actions,)\n",
    "\n",
    "        self.reward = 0\n",
    "        self.state = None\n",
    "            \n",
    "            \n",
    "    def define_rewards(self):\n",
    "        if(self.dialog_state == 4):\n",
    "            reward = 1\n",
    "            return reward\n",
    "        elif(self.dialog_state == 3):\n",
    "            reward = -0.1\n",
    "            return reward\n",
    "        else:\n",
    "            reward = 0\n",
    "            return reward\n",
    "        \n",
    "    def allowed_actions(self, action_probs):\n",
    "        actions = []\n",
    "        allowed_action_probs = []\n",
    "        state = self.state\n",
    "        dialog_state = state[3]\n",
    "        if (dialog_state == 0):\n",
    "            for key, value in self.action_dict[\"open\"]['young_adult'].items():\n",
    "                actions.append(value)\n",
    "            allowed_action_probs = action_probs[actions]\n",
    "            return actions, allowed_action_probs\n",
    "        elif (dialog_state == 1 and state[2] == 1 ):\n",
    "            for key, value in self.action_dict['insured_pos']['young_adult'].items():\n",
    "                actions.append(value)\n",
    "            allowed_action_probs = action_probs[actions]\n",
    "            return actions, allowed_action_probs\n",
    "        elif (dialog_state == 1 and state[2] == 0 ):\n",
    "            for key, value in self.action_dict['insured_neg']['young_adult'].items():\n",
    "                actions.append(value)\n",
    "            allowed_action_probs = action_probs[actions]\n",
    "            return actions, allowed_action_probs\n",
    "        elif (dialog_state == 2):\n",
    "            for key, value in self.action_dict[\"close\"]['all'].items():\n",
    "                actions.append(value)\n",
    "            allowed_action_probs = action_probs[actions]\n",
    "            return actions, allowed_action_probs\n",
    "        else:\n",
    "            print(\"Dialog state is wrong or terminal state is reached\")\n",
    "            return actions, allowed_action_probs\n",
    "    \n",
    "    def reset(self):\n",
    "        print(\"_______ENV RESET___________ \")\n",
    "        self.state = [0,0,0,0,0]\n",
    "        self.dialog_state = 0\n",
    "        self.is_insured = 0\n",
    "        self.prev_response = 0\n",
    "        return np.array(self.state).flatten()\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        if((state[3] == 3) | (state[3] == 4)):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def step(self, action, user):\n",
    "        print(\"Bot said \",action)\n",
    "        next_dialog_state, prev_response,is_insured, reward = user.respond(action)\n",
    "        self.reward = reward\n",
    "        self.dialog_state = next_dialog_state\n",
    "        self.prev_response = prev_response\n",
    "        self.is_insured = is_insured\n",
    "        self.state = [self.user_class,\n",
    "                        self.item_class,\n",
    "                        self.prev_response,\n",
    "                        self.dialog_state,\n",
    "                        self.is_insured]\n",
    "        done = self.is_terminal(self.state)\n",
    "        if done == 1:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        return np.array(self.state), done, self.reward\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determines how to assign values to each state, i.e. takes the state\n",
    "# and action (two-input model) and determines the corresponding value\n",
    "class ActorCritic:\n",
    "\tdef __init__(self, env, sess):\n",
    "\t\tself.env  = env\n",
    "\t\tself.sess = sess\n",
    "\n",
    "\t\tself.learning_rate = 0.001\n",
    "\t\tself.epsilon = 1.0\n",
    "\t\tself.epsilon_decay = .995\n",
    "\t\tself.gamma = .95\n",
    "\t\tself.tau   = .125\n",
    "\n",
    "\t\t# ===================================================================== #\n",
    "\t\t#                               Actor Model                             #\n",
    "\t\t# Chain rule: find the gradient of chaging the actor network params in  #\n",
    "\t\t# getting closest to the final value network predictions, i.e. de/dA    #\n",
    "\t\t# Calculate de/dA as = de/dC * dC/dA, where e is error, C critic, A act #\n",
    "\t\t# ===================================================================== #\n",
    "\n",
    "\t\tself.memory = deque(maxlen=2000)\n",
    "\t\tself.actor_state_input, self.actor_model = self.create_actor_model()\n",
    "\t\t_, self.target_actor_model = self.create_actor_model()\n",
    "\n",
    "\t\tself.actor_critic_grad = tf.placeholder(tf.float32, \n",
    "\t\t\t[None, self.env.N_actions]) # where we will feed de/dC (from critic)\n",
    "\t\t\n",
    "\t\tactor_model_weights = self.actor_model.trainable_weights\n",
    "\t\tself.actor_grads = tf.gradients(self.actor_model.output, \n",
    "\t\t\tactor_model_weights, -self.actor_critic_grad) # dC/dA (from actor)\n",
    "\t\tgrads = zip(self.actor_grads, actor_model_weights)\n",
    "\t\tself.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)\n",
    "\n",
    "\t\t# ===================================================================== #\n",
    "\t\t#                              Critic Model                             #\n",
    "\t\t# ===================================================================== #\t\t\n",
    "\n",
    "\t\tself.critic_state_input, self.critic_action_input, \\\n",
    "\t\t\tself.critic_model = self.create_critic_model()\n",
    "\t\t_, _, self.target_critic_model = self.create_critic_model()\n",
    "\n",
    "\t\tself.critic_grads = tf.gradients(self.critic_model.output, \n",
    "\t\t\tself.critic_action_input) # where we calcaulte de/dC for feeding above\n",
    "\t\t\n",
    "\t\t# Initialize for later gradient calculations\n",
    "\t\tself.sess.run(tf.initialize_all_variables())\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                              Model Definitions                            #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef create_actor_model(self):\n",
    "\t\tstate_input = Input(shape=self.env.state_dim)\n",
    "\t\th1 = Dense(24, activation='relu')(state_input)\n",
    "\t\th2 = Dense(48, activation='relu')(h1)\n",
    "\t\th3 = Dense(24, activation='relu')(h2)\n",
    "\t\toutput = Dense(self.env.N_actions, activation='relu')(h3)\n",
    "\t\t\n",
    "\t\tmodel = Model(input=state_input, output=output)\n",
    "\t\tadam  = Adam(lr=0.001)\n",
    "\t\tmodel.compile(loss=\"mse\", optimizer=adam)\n",
    "\t\treturn state_input, model\n",
    "\n",
    "\tdef create_critic_model(self):\n",
    "\t\tstate_input = Input(shape=self.env.state_dim)\n",
    "\t\tstate_h1 = Dense(24, activation='relu')(state_input)\n",
    "\t\tstate_h2 = Dense(48)(state_h1)\n",
    "\t\t\n",
    "\t\taction_input = Input(shape=self.env.N_actions)\n",
    "\t\taction_h1    = Dense(48)(action_input)\n",
    "\t\t\n",
    "\t\tmerged    = Add()([state_h2, action_h1])\n",
    "\t\tmerged_h1 = Dense(24, activation='relu')(merged)\n",
    "\t\toutput = Dense(1, activation='relu')(merged_h1)\n",
    "\t\tmodel  = Model(input=[state_input,action_input], output=output)\n",
    "\t\t\n",
    "\t\tadam  = Adam(lr=0.001)\n",
    "\t\tmodel.compile(loss=\"mse\", optimizer=adam)\n",
    "\t\treturn state_input, action_input, model\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                               Model Training                              #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef remember(self, cur_state, action, reward, new_state, done):\n",
    "\t\tself.memory.append([cur_state, action, reward, new_state, done])\n",
    "\n",
    "\tdef _train_actor(self, samples):\n",
    "\t\tfor sample in samples:\n",
    "\t\t\tcur_state, action, reward, new_state, _ = sample\n",
    "\t\t\tpredicted_action = self.actor_model.predict(cur_state)\n",
    "\t\t\tgrads = self.sess.run(self.critic_grads, feed_dict={\n",
    "\t\t\t\tself.critic_state_input:  cur_state,\n",
    "\t\t\t\tself.critic_action_input: predicted_action\n",
    "\t\t\t})[0]\n",
    "\n",
    "\t\t\tself.sess.run(self.optimize, feed_dict={\n",
    "\t\t\t\tself.actor_state_input: cur_state,\n",
    "\t\t\t\tself.actor_critic_grad: grads\n",
    "\t\t\t})\n",
    "            \n",
    "\tdef _train_critic(self, samples):\n",
    "\t\tfor sample in samples:\n",
    "\t\t\tcur_state, action, reward, new_state, done = sample\n",
    "\t\t\tif not done:\n",
    "\t\t\t\ttarget_action = self.target_actor_model.predict(new_state)\n",
    "\t\t\t\tfuture_reward = self.target_critic_model.predict(\n",
    "\t\t\t\t\t[new_state, target_action])[0][0]\n",
    "\t\t\t\treward += self.gamma * future_reward\n",
    "\t\t\tself.critic_model.fit([cur_state, action], reward, verbose=0)\n",
    "\t\t\n",
    "\tdef train(self):\n",
    "\t\tbatch_size = 32\n",
    "\t\tif len(self.memory) < batch_size:\n",
    "\t\t\treturn\n",
    "\n",
    "\t\trewards = []\n",
    "\t\tsamples = random.sample(self.memory, batch_size)\n",
    "\t\tself._train_critic(samples)\n",
    "\t\tself._train_actor(samples)\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                         Target Model Updating                             #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef _update_actor_target(self):\n",
    "\t\tactor_model_weights  = self.actor_model.get_weights()\n",
    "\t\tactor_target_weights = self.target_critic_model.get_weights()\n",
    "\t\t\n",
    "\t\tfor i in range(len(actor_target_weights)):\n",
    "\t\t\tactor_target_weights[i] = actor_model_weights[i]\n",
    "\t\tself.target_critic_model.set_weights(actor_target_weights)\n",
    "\n",
    "\tdef _update_critic_target(self):\n",
    "\t\tcritic_model_weights  = self.critic_model.get_weights()\n",
    "\t\tcritic_target_weights = self.critic_target_model.get_weights()\n",
    "\t\t\n",
    "\t\tfor i in range(len(critic_target_weights)):\n",
    "\t\t\tcritic_target_weights[i] = critic_model_weights[i]\n",
    "\t\tself.critic_target_model.set_weights(critic_target_weights)\t\t\n",
    "\n",
    "\tdef update_target(self):\n",
    "\t\tself._update_actor_target()\n",
    "\t\tself._update_critic_target()\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                              Model Predictions                            #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef act(self, cur_state):\n",
    "\t\tself.epsilon *= self.epsilon_decay\n",
    "\t\tif np.random.random() < self.epsilon:\n",
    "\t\t\treturn self.env.action_space.sample()\n",
    "\t\treturn self.actor_model.predict(cur_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\tsess = tf.Session()\n",
    "\tK.set_session(sess)\n",
    "\tenv = gym.make(\"Pendulum-v0\")\n",
    "\tactor_critic = ActorCritic(env, sess)\n",
    "\n",
    "\tnum_trials = 10000\n",
    "\ttrial_len  = 500\n",
    "\n",
    "\tcur_state = env.reset()\n",
    "\taction = env.action_space.sample()\n",
    "\twhile True:\n",
    "\t\tenv.render()\n",
    "\t\tcur_state = cur_state.reshape((1, env.observation_space.shape[0]))\n",
    "\t\taction = actor_critic.act(cur_state)\n",
    "\t\taction = action.reshape((1, env.action_space.shape[0]))\n",
    "\n",
    "\t\tnew_state, reward, done, _ = env.step(action)\n",
    "\t\tnew_state = new_state.reshape((1, env.observation_space.shape[0]))\n",
    "\n",
    "\t\tactor_critic.remember(cur_state, action, reward, new_state, done)\n",
    "\t\tactor_critic.train()\n",
    "\n",
    "\t\tcur_state = new_state\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
